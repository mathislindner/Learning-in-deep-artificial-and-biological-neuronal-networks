Tutorial 5.2 Theory Questions


Name: Mathis Lindner
Student ID: 17-943-960
Email: mlindner@ethz.ch


Question 3.1: When changing the output readings from mean membrane potential to mean number of spikes, what would you expect to happen wrt learning if we did not use the surrogate gradient?

***** NO CLUE IF THIS IS CORRECT *****
Answer: We would have more trouble to backpropagate the error through the network, since the number of spikes is not necessarily an absolute indicator of the membrane potential which is the actual reason why a neuron fires. 
This would make it harder to learn the correct weights.


Question 3.2: In our current implementation, does the computation of the activation of a neuron in layer l+1 at time t occurs before or after the computation of the activation of a neuron in layer l at time t+1?

Answer: We can observe in the spiking_layer.py file, that we "Compute hidden layer state at each time step" & since the function forward() "Computes the output activation of a spiking layer.":
We can conclude that the computation of the activation of a neuron in layer l at time t+1 occurs before the computation of the activation of a neuron in layer l+1 at time t.










